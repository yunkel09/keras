---
title: Redes Neuronales - Autos
subtitle: Ejercicio Obligatorio
author:
- name: William Chavarría
  affiliation: Máxima Formación
  email: wchavarria@tigo.com.gt
date: '`r format(Sys.Date())`'
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
    highlight: pygments
    theme: spacelab
    css: custom.css
    fig_caption: true
    df_print: paged
bibliography: [paquetes_autos.bib, autos.bib]
biblio-style: "apalike"
link-citations: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo        = TRUE, 
                      include     = TRUE,
                      cache       = FALSE,
                      fig.align   = 'center',
                      message     = FALSE,
                      warning     = FALSE, 
                      comment     = NA, 
                      highlight   = TRUE,
                      strip.white = TRUE,
                      dev         = "svglite",
                      fig.width   = 8,
                      fig.asp     = 0.618,
                      fig.show    = "hold",
                      fig.align   = "center")
```

# Autos {.tabset .tabset-fade .tabset-pills}

## Descripción

Analiza el consumo de gasolina para 392 vehículos según sus características. Los
datos se encuentran en el objeto Auto del paquete ISLR y fueron utilizados en
1983 en la American Statistical Association Exposition.

Realice los siguientes pasos:

1. Observa y grafica los datos. 
2. Transforma los datos cuando sea necesario. 
3. Divide el conjunto de datos en uno de entrenamiento y otro de prueba. 
4. Construye el modelo NN, grafica e interpreta el resultado. 
5. Evalúa la performance del modelo NN. 

Interprete los resultados. Los datos se encuentran dentro de la librería
kernlab que debemos instalar (install.packages()) y cargar (library()). Para
cargar los datos se debe utilizar data(autos).

## Paquetes

```{r}
options(warn = -1,
		  dplyr.summarise.inform = FALSE,
		  tibble.print_min = 5,
		  tidymodels.dark = TRUE,
		  readr.show_col_types = FALSE)
```

```{r}
import::from(magrittr, "%T>%", "%$%",  "%<>%", .into = "operadores")
import::from(cowplot, .except = "stamp")
import::from(kableExtra, .except = "group_rows")
import::from(DataExplorer, plot_intro, plot_bar, plot_density)
import::from(parallel, detectCores, makePSOCKcluster, stopCluster)
import::from(dviz.supp, theme_dviz_open)
import::from(doParallel, registerDoParallel)
import::from(DescTools, JarqueBeraTest)
import::from(weights, rd, starmaker)
import::from(colorspace, scale_fill_continuous_divergingx)
import::from(corrplot, corrplot)
import::from(bestNormalize, bestNormalize, step_best_normalize)
import::from(tidytext, reorder_within, scale_y_reordered, scale_x_reordered)
import::from(nortest, ad.test, pearson.test, sf.test, lillie.test)
import::from(conectigo, cargar_fuentes)
import::from(colorblindr, scale_color_OkabeIto, palette_OkabeIto, scale_fill_OkabeIto)
import::from(GGally, ggpairs, wrap)
import::from(patchwork, plot_layout, plot_annotation)
import::from(janitor, make_clean_names, clean_names)
pacman::p_load(pins,
					moments,
					ggridges,
					inspectdf,
					skimr,
					finetune,
					parsnipExtra,
					neuralnet,
					NeuralNetTools,
					tensorflow,
					keras,
					embed,
					tictoc,
					textrecipes,
					tidymodels,
					tidyverse)
```

## Funciones

```{r}
tabla <- function(df, cap = "prueba") {
  
  df %>% 
   kbl(booktabs = TRUE, caption = cap, escape = F) %>% 
   kable_paper(lightable_options = "hover", full_width = F)}
```

```{r}
resaltar <- function(texto) {
    
    glue::glue("<span style='background-color: #FFFF00'>**{texto}**</span>")
    
}
```

```{r}
rlt <- function(texto, color) {
    
	a <- "<span style='background-color: "
	b <- "'>"
	c <- "</span>"
	t <- str_c("**", texto, "**")
	
	f <- str_c(a, color, b)
   
	glue::glue(f, t, c) 
	
    
}
```

```{r}
# detener el backend
unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
```

```{r}
# resumir estadísticos principales
resumir <- function(.df) {
 my_skim <- skim_with(
  base = NULL,
  numeric = sfl(media   = ~ mean(., na.rm = TRUE),
                mediana = ~ median(., na.rm = TRUE),
                maximo  = ~ max(., na.rm = TRUE),
                minimo  = ~ min(., na.rm = TRUE),
  				    skewness = ~ skewness(.x),
                kurtosis = ~ kurtosis(.x)), append = F)
 my_skim(.df) |>
 rename_with(~ str_replace_all(.x, "numeric\\.", "")) |>
 as_tibble() |>
 rename(tipo = skim_type, variable = skim_variable) |> 
 clean_names()
}

```
 
```{r, include=FALSE}
# colorear fuente
colf <- function (x, color) {
   
	t <- str_c("**", x, "**")
	paste("<font color='", color, "'>", t, "</font>", sep = "")
	
}
``` 
 
```{r}
barra <- function(df, x) {
	
	dfx <- df %>%
		tabyl({{x}}) %>% 
		adorn_pct_formatting()
	
	dfx %>% 
		ggplot(aes(y = {{x}}, x = n)) +
		geom_col(fill = "#0072B2", width = 0.8) +
		geom_text(aes(label = str_c(n, " ", "(", percent, ")")),
					 hjust = 1.5,
					 size = 6,
					 color = "white") +
					 # fontface = "bold") +
		scale_x_continuous(name = NULL, expand = c(0, 0)) +
		scale_y_discrete(name = NULL, expand = c(0, 0.5)) +
		# coord_cartesian(clip = "off") +
		theme_minimal_vgrid(font_family = "yano") +
		theme(axis.text.y = element_text(size = 14),
				plot.title = element_text(size = 22, face = "bold"))
}
```


```{r}
# agregar línea loess a las gráficas ggpairs
loess_lm <- function(data, mapping, ...){
 
ggplot(data = data, mapping = mapping) + 
    geom_point(alpha = 0.9) + 
    stat_smooth(formula = y ~ x, 
                method = "lm", 
                se = TRUE, 
                color = "blue",
                fill = "blue",
                size = 0.5, 
                alpha = 0.2,
                linetype = "longdash", 
                ...)
}
```


```{r}
# crear qq-plots
qpl <- function(df, var_y, rel) { 
      
      df %>% 
       ggplot(aes(sample = .data[[var_y]])) +
       qq$geom_qq_band(bandType = "pointwise", 
                       distribution = "norm", 
                       alpha = 0.5) +
       qq$stat_qq_line() +
       qq$stat_qq_point(size   = 2, 
                        shape  = 21, 
                        alpha  = 0.8, 
                        fill   = rel, 
                        colour = rel) +
       labs(x = "Theoretical Quantiles", y = "Sample Quantiles") +
       ggtitle(str_to_title(var_y)) +
       theme(plot.title = element_text(size = 16))
      
     }
```


```{r}
# pruebas de normalidad no paramétrica
funciones <- list(
 
  shapiro_wilk       = function(x) shapiro.test(x),
  jarque_bera        = function(x) JarqueBeraTest(x, robust = F),
  pearson            = function(x) pearson.test(x), 
  shapiro_francia    = function(x) sf.test(x),
  kolgomoro_smirnov  = function(x) lillie.test(x)
)
```

```{r}
# aplicar pruebas de normalidad
probar_normalidad <- function(vector) {
	
	funciones %>% 
		map(exec, x = vector) %>% 
		map_df(tidy) %>% 
		select(method, p_value = p.value) %>% 
		# mutate(normalidad = ifelse(p_value < 0.05, "NO_NORMAL", "NORMAL")) %>% 
		arrange(desc(p_value))
}
```

```{r}
# generar gráfico de densidad
estimar_densidad <- function(df, d, color) {
	
	brk <- hist(df[[d]], plot = FALSE)$breaks 
	med <- mean(df[[d]])
	
	df %>% 
	  ggplot(aes(x = .data[[d]], y = ..density..)) +
	  geom_histogram(fill   = color,
	                 colour = "black",
	                 size   = .2,
	                 breaks = brk) +
	  scale_x_continuous(name   = d,
	                     breaks = brk) +
	  geom_density(size = 1) +
	  geom_vline(xintercept = med, 
	             linetype = "dashed",
	             color = "red", 
	             alpha = 0.5) 
}
```


## Opciones

```{r}
set.seed(2022)
```

```{r}
colorx <- c(rojo = "#F4354D", amarillo = "#FCA108", verde = "#00AB40")
```


```{r}
cargar_fuentes()
```

```{r}
yunkel <- theme_cowplot(font_family = "yano") +
	       theme(plot.margin = unit(c(3, 1, 1, 1), "mm"), 
	             axis.title = element_text(size = 12))
```

```{r}
# tema con grid horizontal y vertical
drako <- theme_bw(base_family = "yano", base_size = 14) +
	      theme(plot.margin = unit(c(6, 1, 1, 1), "mm"),
	            axis.title = element_text(size = 12),
	            plot.subtitle = element_text(size = 8,
                                            family = "sans"))
```

```{r}
# tema para histogramas binned
furia <- yunkel +
 theme(axis.line = element_blank(),
       panel.grid.major.y = element_line(color = "#e5e5e5"))
```

```{r}
ver <- . %>% prep() %>% juice()
```

```{r}
theme_set(yunkel)
```

# Carga

Cargar los datos y transformar aquellas variables que se consideren factor.

```{r}
autos_raw <- ISLR::Auto |> as_tibble()
```


# Análisis Exploratorio

## Estructura

```{r}
autos_raw |> sample_n(size = 10) |> 
	tabla(cap = "Observaciones")
```

</br>

<p class="comment">
Se observa que la columna name contiene la marca y el modelo  y
el acabado Podemos extraerlo como parte del *feature engineering* 
</p>


## Feature Extraction


```{r}
pro <- autos_raw |> 
 mutate(across(name, as.character)) |> 
 separate(name, c("marca", "modelo", "acabado"),
          extra = "drop", fill = "right")
```

```{r}
pro |> slice_sample(n = 5) |> tabla(cap = "Missing values en variable acabado")
```

</br>

## Missing Values

```{r}
pro %>% inspect_na() |> tabla("Valores perdidos")
```

</br>

Vemos que hay valores perdidos en la columna *acabado*

El nivel de equipamiento del modelo **base** de un fabricante de automóviles es
la versión más simple del nuevo vehículo. Un modelo base representa la variación
de modelo menos costosa del vehículo que ofrece el fabricante de automóviles.
**Es posible que el nivel de equipamiento de un modelo base no tenga un nombre
específico**, pero el mismo modelo de automóvil puede tener varios niveles de
equipamiento.^[ver https://bit.ly/3wyko1N].  Por esta razón, agregaremos el
valor "base" a aquellos vehículos que no tengan especificación (NA) en la
columna *acabado.*

```{r}
# vector de reemplazo
mk_00 <- c(`1` = "american",  `2` = "european", `3` = "japanese")
```

```{r}
autos <- autos_raw |> 
 mutate(across(name, as.character)) |> 
 separate(name, c("marca", "modelo", "acabado"),
          extra = "drop", fill = "right") |> 
 mutate(
  across(acabado, replace_na, "base"),
  across(origin, recode, !!!mk_00),
  across(where(is.character), as.factor),
  across(c(cylinders, year), as.factor))
```

```{r}
autos |> slice_sample(n = 10) |> tabla(cap = "Feature Extraction")
```


```{r, include=FALSE}
# nombres de variables con colores
w <- names(autos) %>% 
	set_names(.) %>% 
	map_chr(~ colf(.x, "#93330E"))
```

</br>

<p class="comment">
Hemos sustituidos los `r colf("NA", colorx[["rojo"]])` por **base**.
</p>


```{r}
autos |>
	inspect_na() |> 
	tabla(cap = "Valores perdidos")
```

</br>

Aun se observan dos valores perdidos. Estos deberán trabajarse en la fase de
pre-procesamiento.

Ahora que ya hemos realizado el *feature extraction*, procedamos a revisar la
estructura de nuestro dataset

```{r}
plot_intro(autos, ggtheme = yunkel, title = "Resumen")
```

<br/>

La mayoría de las columnas son numéricas continuas. Hay una pequeña cantidad
de valores perdidos.

```{r}
est <- resumir(autos)
```

```{r}
est_num <- est |> 
 filter(tipo == "numeric") |> 
 select(variable, media:kurtosis)
```

```{r}
est_fac <- est |>
 filter(tipo == "factor") |> 
 select(variable:factor_top_counts)
```

Veamos un resumen de las variables numéricas:

```{r}
est_num
```

<p class="comment">
Sabemos que el *skewness* en una distribución normal es igual a cero, así que
valores muy cercanos a cero indican distribuciones simétricas. Con excepción
de `r w[4]` la mayoría de los predictores (incluyendo la respuesta) se ven
bastante simétricos.  
</p>

</br>

Ahora de las categóricas

```{r}
est_fac
```

</br>

<p class="comment">
Vemos que las variables recién creadas `r w[9]`, `r w[10]` y `r w[11]` presentan
una **alta cardinalidad**.  Será necesario aplicar técnicas de codificación un
poco más avanzadas. En este caso crear variables *dummy* o aplicar *One-Hot
Encoding* podría crearnos un dataset con alta dimensionalidad (ver maldición de
la dimensionalidad)^[A medida que crece el número de características, la
cantidad de datos que necesitamos para poder distinguir con precisión entre
estas características (para darnos una predicción) y generalizar nuestro modelo
(función aprendida) crece EXPONENCIALMENTE.]
</p>

## Cardinalidad

(ref:gr-01) Alta cardinalidad

```{r, gr-01, fig.cap='(ref:gr-01)'}
autos |> select(where(is.factor)) |>
	inspect_cat() |> 
	show_plot(high_cardinality = 4, col_palette = 1)
```

<br/>

En el gráfico \@ref(fig:gr-01) podemos comprobar visualmente que contamos con
una alta cardinalidad en nuestros features categóricos recién creados.

## Respuesta

Debido a que el objetivo más importante del modelado es entender la variación en
la respuesta, el primer paso debería ser entender la distribución de esta.

```{r}
brk <- hist(autos$mpg, plot = F)$breaks
res <- resumir(autos$mpg)
```

```{r}
autos |> 
 ggplot(aes(x = mpg, y = ..density..)) +
 geom_histogram(fill = "#56B4E9", size = .2, breaks = brk, color = "white") +
 geom_density(size = 1) +
 geom_vline(xintercept = 0, color = "black", linetype = "dashed") +
 geom_vline(xintercept = res[["media"]], color = "black", linetype = "dashed") +
 geom_vline(xintercept = res[["mediana"]], color = "green", linetype = "dashed") +
 scale_x_continuous(name = "MPG",
                    expand = c(0, 0),
                    limits = c(0, 55),
                    breaks = brk) +
 labs(title = "Consumo en Millas por Galón") + furia
```

<p class="comment">
El histograma muestra un ligero sesgo hacia la derecha, con la media
desplazándose ligeramente con respecto a la mediana.  Diera la impresión de que
hay dos picos en la distribución. Todos los valores de `r w[1]` son positivos.
No se observan valores atípicos.  No podemos concluir visualmente que la
distribución del a respuesta sea normal.
</p>

Realizaremos pruebas formales en un apartado posterior

```{r}
bandwidth <- 2.68
```

(ref:gr-02) Análisis de la respuesta por año de origen

```{r, gr-02, fig.cap='(ref:gr-02)'}
autos |> 
 ggplot(aes(x = mpg, y = year, fill = stat(x))) +
 geom_density_ridges_gradient(
 	scale = 3, rel_min_height = 0.01,
 	bandwidth = bandwidth) +
 scale_fill_viridis_c(option = "C") +
 scale_x_continuous(name = "MPG", breaks = seq(0, 70, 5)) +
 scale_y_discrete(name = "Año", expand = c(0, .2, 0, 2.6)) +
 theme_ridges(font_family = "yano") +
 labs(title = "MPG por año de origen")
```

<br/>

<p class="comment">
En el gráfico \@ref(fig:gr-02) vemos que una explicación razonable para la
variación en la repuesta es que al pasar los años, el consumo `r w[1]` se fue
mejorando. Vemos que la diferencia entre la media de `r w[1]` en el año 70 es
mucho menor que la media en el año 82.
</p>

## Predictores

```{r}
autos_n <- autos %>% select(where(is.numeric), -mpg)
pal    <- palette_OkabeIto[1:ncol(autos_n)]
ndv    <- rev(names(autos_n))
```

(ref:densidad) Histograma con densidad

```{r, densidad, fig.cap='(ref:densidad)', fig.width=11, fig.asp=0.7}
map2(.x = ndv, .y = pal, ~ estimar_densidad(df = autos_n, d = .x, color = .y)) |> 
 reduce(.f = `+`) + 
 plot_layout(ncol = 2) +
 plot_annotation(title    = "Distribución", 
                 subtitle = "Estimación de densidad no paramétrica") 
```
\

En la figura \@ref(fig:densidad) vemos la distribución de los predictores
numéricos. 

* `r w[6]`: El *skewness* que observamos en el resumen numérico nos confirma lo
que se aprecia en el gráfico. La distribución se ve bastante simétrica, con una
*kurtosis* de 3.4^[Una distribución normal tiene una kurtosis de 3].

* `r w[5]`: Es posible que este predictor siga una distribución *log-normal*
o *gamma*. Habrá que considerarlo en el proceso de transformaciones.

* `r w[4]`: Se ve bimodal y con sesgo positivo.

* `r w[3]`: Al igual que el anterior se ve un decrecimiento exponencial a medida
que aumentan los valores de la respuesta toma valores más alto.


```{r, normalidad}
autos %>% 
 select(where(is.numeric)) |> 
 imap_dfr(~ probar_normalidad(.x) %>%
 			mutate(var = .y)) %>% 
 pivot_wider(names_from = "var", values_from = "p_value") %>% 
 mutate(across(where(is.numeric), rd, 2)) %>% 
 tabla(cap = "Pruebas no paramétricas de normalidad")
```
\

<p class="comment">
En la tabla \@ref(tab:normalidad) se realizaron varias pruebas no
paramétricas y comprobamos que **la variable respuesta no es normal.**. Lo
mismo para el resto de variables numéricas. Pensamos que la excepción podría
ser `r w[6]`, sin embargo, ninguna prueba confirma normalidad.
</p>

## Atípicos

(ref:gr-03) Valores atípicos

```{r, gr-03, fig.cap='(ref:gr-03)'}
autos_n |> 
 pivot_longer(cols = where(is.numeric),
					 names_to = "variable",
					 values_to = "valor") |> 
 ggplot(aes(y = valor, x = fct_reorder(variable, valor, .desc = T))) +
 geom_boxplot(aes(fill = variable), outlier.color = "red") +
 scale_fill_OkabeIto() +
 xlab("variable") +
 scale_y_log10() +
 theme(legend.position = "none") +
 labs(title = "Variabilidad y rango de predictores numéricos")
```

<br/>

<p class="comment">
En el gráfico \@ref(fig:gr-03) se observa que los predictores numéricos no se
encuentran en la misma escala, por lo que será necesario realizar *feature
scaling*. Otra punto interesante es que el feature con menos variación y el que
más se aproxima a una distribución normal es el que presenta valores atípicos.
Deberemos revisar estos valores antes de proceder con el modelado.
</p>

## Categóricas

(ref:gr-04) Recuento variables categóricas

```{r, gr-04, fig.cap='(ref:gr-04)', fig.width=12, fig.height=15}
plot_bar(autos, by = "origin", 
			with = "mpg", 
			by_position = "dodge",
			ggtheme = yunkel)
```

<br/>

<p class="comment">
En el gráfico \@ref(fig:gr-04) se observan las variables categóricas por
frecuencia. En el caso de las variables `r w[9]`, `r w[10]` y `r w[11]`, a como
lo observamos al inicio tienen una alta cardinalidad.  Estas variables debemos
veras con un *cleveland dot plot*.  El aumento y disminución de `r w[1]` es
con base a la cantidad de cilindros, el año y la marca es un claro indicio de
que son variables que influyen en la respuesta.
</p>

(ref:gr-05) MPG por marca

```{r, gr-05, fig.cap='(ref:gr-05)', fig.asp = 1}
autos |> 
 group_by(marca) |> 
 summarise(media = mean(mpg), std_err = sd(mpg) / sqrt(length(mpg))) |> 
 ggplot(aes(x = media, y = fct_reorder(marca, media))) +
 geom_errorbar(aes(xmin = media - 1.64 * std_err, xmax = media + 1.64 * std_err)) + 
 geom_point(color = "#0072B2", size = 3) +
 scale_y_discrete(name = NULL) + drako
```

<br/>

<p class="comment">
En el gráfico \@ref(fig:gr-05) hay dos cosas importantes que notar. La primera
es que vemos que hay nombres que no están escritos correctamente. Por ejemplo,
existe *vokswagen* y *volkswagen*.  Será necesario corregir esto como parte del
*data cleaning*.  Lo segundo a notar es la variación que tienen algunas marcas
con respecto al nivel de combustible.
</p>

```{r}
autos |> 
 count(marca) |> 
 arrange(marca) |> print(n = Inf)
```

```{r}
mk_01 <- c(
		"chevroelt" = "chevrolet",
		"maxda"     = "mazda",
		"toyouta"   = "toyota",
		"vokswagen" = "volkswagen")
```

```{r}
autos %<>% mutate(across(marca, recode, !!!mk_01))
```

<p class="comment">
Hemos realizado la limpieza de las marcas de vehículos.  También se pudo haber
realizado un **stemming** en la parte de preprocesamiento, sin embargo, no
hubiera sido tan óptimo debido a que estas son marcas de vehículos y no palabras
cotidianas.
</p>


```{r}
autos_sum <- autos |> 
 group_by(marca) |> 
 summarise(
 	year  = min(as.integer(as.character(year))),
 	mpg   = mean(mpg), 
 	.groups = "drop")
```

(ref:gr-06) Heatmap

```{r, gr-06, fig.cap='(ref:gr-06)', fig.asp = 0.9}
autos_sum |> 
 mutate(across(year, as.factor)) |> 
 ggplot(aes(x = year, y = marca, fill = mpg)) +
 geom_tile(color = "white", size = 0.25) +
 scale_fill_viridis_c(
    option = "A", begin = 0.05, end = 0.98,
    limits = c(0, 100),
    name = "mpg",
    guide = guide_colorbar(
      direction = "horizontal",
      label.position = "bottom",
      title.position = "top",
      ticks = FALSE,
      barwidth = grid::unit(3.5, "in"),
      barheight = grid::unit(0.2, "in")
    )) +
  scale_x_discrete(expand = c(0, 0), name = NULL) +
  scale_y_discrete(name = NULL, position = "left") +
  yunkel +
  theme(
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    axis.ticks.length = grid::unit(1, "pt"),
    legend.position = "top",
    legend.justification = "left",
    legend.title.align = 0.5,
    legend.title = element_text(size = 12*12/14)
  )
```

<br/>

<p class="comment">
En el gráfico \@ref(fig:gr-06) vemos una relación entre `r w[9]` y `r w[1]`.
La variación en `r w[1]` se ve afectada por las diferentes marcas. Esto se
observa a partir de la variación en los colores.  También el año podría ser un
predictor importante ya que se observa que la tonálidad va aclarándose a medida
que aumentan los años.
</p>

## Correlación

(ref:cormatriz) Matriz de correlación

```{r, cormatriz, fig.cap='(ref:cormatriz)'}
corm <- cor(autos_n)
corrplot(corr = corm,
			method = "color",
			order = "hclust",
			type = "upper",
         addCoef.col = "black",
         outline = F,
         diag = TRUE, 
         col = colorRampPalette(c("deepskyblue1","white","indianred3"))(100), 
         tl.cex = 0.8, number.cex = 1, cl.cex = 1, tl.col = "black", 
         tl.pos = "td", tl.srt = 45)
```

```{r}
caret::findCorrelation(corm, cutoff = 0.75)
```

(ref:matriz) Gráfico de correlación

```{r, matriz, fig.cap='(ref:matriz)', fig.width=12, fig.asp=0.7}
autos %>%
	select(where(is.numeric)) %>% 
	ggpairs(., lower = list(continuous = loess_lm),
 		     upper = list(continuous = wrap("cor", size = 5))) + drako
```
\
<p class="comment">
Algunos modelos podrían beneficiarse de una decorrelación (eliminar
colinealidad) de predictores o en palabras más sencillas, cuando dos variables
predictoras están altamente correlacionadas debemos eliminar una de ellas para
eliminar información redundante. El alto grado de correlación es un claro
indicador de que la información presente en los autos es redundante y podría
eliminarse o reducirse. Esto lo realizaremos en el preprocesamiento.
</p>

## Asimetría

Trataremos de identificar la mejor transformación para cada predictor utilizando
la función `step_best_normalize` de [@R-bestNormalize]

```{r}
autos_transformado <- recipe(mpg ~ ., data = autos) %>%
    step_best_normalize(all_predictors(), -all_nominal()) %>%  
    ver() |> 
	 select(-mpg) |> 
    rename_with(~ str_c(.x, "_tr"), is.double) %>% 
    select(where(is.double)) %>% 
    bind_cols(autos %>% select(where(is.double), -mpg))
```

```{r, fig.asp=1}
autos_transformado %>% 
    pivot_longer(cols = displacement_tr:acceleration,
                         names_to = "variable",
                         values_to = "valores") %>% 
    ggplot(aes(x = valores, y = ..density..)) +
    geom_histogram(fill   = "#56B4E9",
                   colour = "black",
                   size   = .2) +
    geom_density(size = 1) +
    facet_wrap(variable ~ ., scales = "free", ncol = 2) +
    labs(title = "Predictores Transformados", subtitle = "BestNormalize") +
    drako
```


# Modelado

## Split

```{r}
autos_split <- initial_split(autos, prop = 0.7, strata = mpg)
autos_train <- training(autos_split)
autos_test  <- testing(autos_split)
```

```{r}
dim(autos_train)
```

## Cross-validación

```{r}
autos_folds <- vfold_cv(autos_train, v = 10, strata = mpg)
```

```{r}
autos_folds$splits[[1]] |> assessment()
```


## Transformaciones

Debido a la alta cardinalidad de los features categóricas, debemos considerar
cual sería el espacio de features que tendrían que analizar los modelos si
aplicamos una técnica convencional tal como *One-Hot Encoding* o convertir a
variables *dummy*.

```{r}
# contar los valores únicos de los features categóricos
(com <- autos |> 
 select(where(is.factor)) |> 
 map_int(~ length(unique(.x))))
```

Ahora calculemos las combinaciones posibles:

```{r}
com |> reduce(`*`)
```

El resultado de todas las posibles combinaciones es demasiado grande. Nuestras
transformaciones deben hacer uso de técnicas más apropiadas para el tratamiento
de esta alta cardinalidad. 

### Recetas

Crearemos dos recetas. La primera será la más simple de todas y solo incluiremos
predictores numéricos, es decir, no incluiremos predictores categóricos. El
objetivo es ver el comportamiento de la red con pocos predictores.

```{r}
simple_rec <- recipe(mpg ~ ., data = autos_train) |> 
 step_select(where(is.numeric)) |> 
 step_corr(all_numeric_predictors(), threshold = 0.9) |> 
 step_range(all_predictors())
```

```{r}
simple_rec |> ver()
```

Para la segunda receta consideramos todos los predictores y se aplicaron más
pasos en las transformaciones. 

```{r}
uni_rec <- recipe(mpg ~ ., data = autos_train) |>
 step_rm(acabado) |>
 step_lencode_mixed(year, cylinders, outcome = vars(mpg)) |>
 step_YeoJohnson(all_numeric(), -all_nominal(), -all_outcomes()) |>
 step_dummy(origin, one_hot = TRUE) |>
 step_corr(all_numeric_predictors(), threshold = 0.9) |> 
 step_impute_mode(modelo) |>
 step_other(marca, modelo, threshold = 0.01) |>
 step_dummy_hash(marca, modelo, signed = TRUE, num_terms = 16L) |>
 step_zv(all_predictors()) |> 
 step_rename_at(everything(), fn = ~ gsub(replacement = "", x = ., 
 													   pattern = "dummyhash_", fixed = TRUE))
```

A continuación todas las transformaciones que se realizarán al momento de
realizar el proceso de remuestreo a través de cross-validación:

```{r}
uni_rec |> tidy() |> select(-id)
```

\

* `step_rm`: En este caso consideraremos que las adecuaciones a los vehículos,
es decir, los acabados, podrían tener un impacto muy pequeño sino que nulo en
el `r w[1]`. Removeremos esta variable del análisis.

* `step_lencode_mixed`: Técnica de *codificación de efecto* en el que el valor
original de la variable categórica se remplaza con un valor que mide el efecto
de esos datos. Estos pasos usan un modelo lineal generalizado para estimar el
efecto de cada nivel en un predictor categórico sobre el resultado. [@R-embed]

* `step_YeoJohnson`: No utilizaremos la función `step_best_normalize` debido a
que la transformación que mejor se adecua para estos predictores es
`step_orderNorm` y estos valores hacen fallar los modelos de redes neuronales 
que estamos estudiando.^[No se logró determinar la causa del porque el modelo
no ajusta con este tipo de transformación, así que elegimos la segunda mejor].

* `step_dummy`: Esta es la codificación de variables categóricas convencional.
En lugar de utilizar el método que aplica una función de *contraste*, decidimos
utilizar el parámetro `one_hot = TRUE`.

* `step_corr`: A como se observó en la sección de *correlación*, hay varios
predictores con una alta correlación entre ellos. Utilizaremos este filtro para
eliminar variables que estén muy correlacionadas entre sí. El valor fue elegido
tratando de que no eliminara demasiados predictores.

* `step_impute_mode`: Imputaremos los valores perdidos de `r w[10]` utilizando
la moda.

* `step_other`: Agrupará valores infrecuentes en las variables categóricas
dentro de un grupo llamado "otros".  Aunque el *threshold* es *tuneable*, no
lo realizaremos por que esto hace que el ajuste tarde todavía más. Utilizarmos
el mismo umbral para las variables `r w[9]` y `r w[10]`.

* `step_dummy_hash`: Aquí aplicaremos *feature hashing*. Esta técnica tiene
muchas desventajas en cuanto a la interpretabilidad y a los problemas con las
denominadas colisiones^[ver https://bit.ly/3LHP3hg], sin embargo, como técnica
de reducción de la dimensionalidad es muy útil ya que podemos definir un
espacio mucho más pequeño de categorías (reducción de cardinalidad).

* `step_zv`: Es probable que algunas columnas hash contengan todos ceros, por lo
que realizaremos un filtro de varianza cero para filtrar dichas columnas.

* `step_rename_at`: Los features creados con el truco del hash tienen un
prefijo que hace que sea más largo el nombre. Lo mejor es eliminar este prefijo.

\

```{r}
uni_rec |> ver()
```

\

Vemos que posterior a las transformaciones ahora tenemos
`r ncol(uni_rec %>% ver())` *features*, las cuales son todas numéricas. Los
valores con -1 los agrega la transformación *feature hashing* cuando hay
colisiones.

## Modelos

Comencemos revisando que modelos tenemos a nuestra disposición: 

```{r}
show_engines("mlp") |> filter(mode == "regression")
```

### Neuralnets

Encontramos una adaptación del modelo *neuralnet* en el paquete {ParsnipExtra}
para que pudiera adaptarse al framework de tidymodels, sin embargo, no podremos
aplicarlo de manera normal a como con el resto de paquetes integrados. La
razón es la siguiente:

El autor del paquete ParsnipExtra no siguió ciertas convenciones que hacen
que el paquete se integre de una manera que el paralelismo se realice de manera
correcta.  Es decir, es posible correr neuralnet sin paralelismo, pero tarda
demasiado. Lo que se hizo fue correrlo sin realizar el *tuning* de los
hiperparámetros.


```{r}
# corrida manual
ruta <- fs::path_wd("09_redes_neuronales", "autos", "modelos")
tablero <- board_folder(path = ruta, versioned = TRUE)

#corrida en knit
# tablero <- board_folder(path = "../modelos")
```

```{r, eval=FALSE, echo=FALSE}
pin_versions(board = tablero, name = "neural_res")
```


#### Ajuste del modelo

```{r}
metricas_neural <- metric_set(rmse, mae)
```

A continuación vamos a inicializar el *motor* de neuralnet a través de la
interfaz de Tidymodels.  Los principales hiperparámetros fueron mapeados de
la función original `neuralnet()`. 

A como se explicó anteriormente, no será posible realizar la optimización de
hiperparámetros, por lo que probaremos con 10 capas y 10 épocas.  El resto
de parámetros específicos de `neuralnet()` se definen en la sección de
`set_engine()`.  Incrementamos el `stepmax` para darle tiempo al algoritmo de
converger.

```{r}
mlp_neural <- mlp(hidden_units = 10, 
						epochs = 10) |>
	set_engine("neuralnet") |>
	set_engine(engine    = "neuralnet",
				  algorithm = "rprop+",
				  stepmax   = 1e6,
				  err.fct   = "sse",
				  act.fct   = "logistic",
				  threshold = 0.1,
				  linear.output = TRUE) |>
	set_mode("regression")
```

```{r}
neural_wflow <- workflow() |> 
 add_model(mlp_neural) |> 
 add_recipe(simple_rec)
```

#### Sin remuestreo


```{r, echo=FALSE}
neural_fixfit <- pin_read(board = tablero, name = "neural_res")
```

```{r, eval=FALSE}
# 100.62 seg
tic()
neural_fixfit <- parsnip::fit(neural_wflow, autos_train)
toc()
```

Es necesario extraer el modelo (clase "nn") del *wrapper* workflow. De esta
manera será posible utilizarlo como si hubiéramos usado directamente la
función `neuralnet()`.

```{r}
neural_ajuste <- neural_fixfit |> extract_fit_engine()
```

```{r}
names(neural_ajuste)
```

```{r}
plotnet(neural_ajuste)
```

Se debe aplicar la misma transformación que tuvieron los datos de entrenamiento
a los datos de prueba:

```{r, eval=FALSE}
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
```

`last_fit()` ajustará el modelo a todo el conjunto de entrenamiento y lo
evaluará con el conjunto de prueba.




```{r, eval=FALSE}
# 253 seg
tic()
final_neural_res <- last_fit(neural_wflow, autos_split)
toc()
```

```{r, eval=FALSE}
stopCluster(cl)
unregister()
```


```{r, eval=FALSE, echo=FALSE}
pin_write(board = tablero,
			 x     = final_neural_res,
			 name  = "final_neural_res",
			 type  = "rds",
			 title = "ajuste al conjunto de prueba",
			 description = "sin resampling")
```

```{r, echo=FALSE, eval=FALSE}
pin_versions(tablero, "final_neural_res")
```


```{r}
wflow_ajustado <- extract_workflow(final_neural_res)
```

Veamos ahora las métricas resultantes para cada pliego (*Fold*) de forma
individual y luego veámosla el promedio general.

```{r}
final_neural_res |> 
 collect_metrics(summarize = F) |> 
 pivot_wider(names_from = .metric, values_from = .estimate) |> 
 select(-c(.estimator, .config)) |> 
 mutate(id = "test_results", .before = 1) |> 
 tabla("Resultados del ajuste")
```


```{r}
collect_predictions(final_neural_res) |> 
 ggplot(aes(x = mpg, y = .pred)) +
 geom_abline(lty = 2) + 
 geom_point(alpha = 0.5) + 
 labs(y = "Predicted MPG", x = "Real MPG") +
 coord_obs_pred()
```

#### Con remuestreo

```{r}
keep_pred <- control_resamples(save_pred = TRUE,
										 save_workflow = TRUE)
```

Con `fit_resamples()` obtendremos estimaciones más realistas que si solo
aplicamos el ajuste de forma directa.

```{r, echo=FALSE, eval=FALSE}
pin_versions(tablero, "neural_res")
```

```{r, echo=FALSE, eval=FALSE}
# ver cual versión debo cargar
ver <- pin_versions(tablero, "neural_res")$version[[10]]
pin_meta(tablero, "neural_res", version = ver)$description
```

```{r, echo=FALSE}
neu_res <- pin_read(board = tablero, name = "neural_res", version = ver)
```

```{r, eval=FALSE}
set.seed(1003)
tic()
neu_res <- neural_wflow |> 
 fit_resamples(
 	resamples = autos_folds,
 	metrics   = metricas_neural,
 	control   = keep_pred)
toc()
```

```{r}
neu_res
```

Vemos que el algoritmo no logró converger en uno de los *Folds* dentro de
la cantidad de pasos que establecimos.

```{r, eval=FALSE, echo=FALSE}
pin_write(board = tablero,
			 x     = neu_res,
			 name  = "neural_res",
			 type  = "rds",
			 title = "neuralnet sin parámetros y con 10 capas y 10 epochs",
			 description = "con resampling y errores")
```

Veamos ahora las métricas resultantes para cada pliego (*Fold*) de forma
individual y luego veámosla el promedio general.

```{r}
neu_res |> 
 collect_metrics(summarize = FALSE) |> 
 pivot_wider(names_from = .metric, values_from = .estimate) |> 
 select(-c(.estimator, .config)) |> 
 tabla("Métricas por pliego")
```

Ahora veamos el promedio de todas las métricas calculadas con cross-validación:

```{r}
neu_res |> 
 collect_metrics(summarize = TRUE) |> 
 select(.metric, mean, .config) |> 
 pivot_wider(names_from = .metric, values_from = mean) |> 
 tabla("Promedio RMSE de todos los pliegos")
```


```{r}
as_workflow_set(h = neu_res) |> 
 autoplot(select_best = TRUE, rank_metric = "rmse") + drako
```

```{r}
assess_res <- neu_res |> collect_predictions()
```

```{r}
assess_res |> slice_sample(n = 10) |> 
	select(-.row, -.config)
```

```{r}
assess_res %>% 
  ggplot(aes(x = mpg, y = .pred)) + 
  geom_point(alpha = .15) +
  geom_abline(color = "red") + 
  coord_obs_pred() + 
  ylab("Predicted")
```

En general la nube de punto está bastante cercana a la diagonal. Hay algunos
puntos que están subestimados y otros sobre-estimados, pero en general no
observamos que hayan puntos que se salgan demasiado.

Para un uso posterior, convertiremos los resultados del *resampling* en un
objeto de clase `workflow_set`.  Esto nos servirá para realizar comparaciones y
otras cosas.

```{r}
neural_res <- as_workflow_set(rec_all_neural = neu_res)
```

```{r}
tune_rank_neural <- neural_res |>
 rank_results(select_best = TRUE, rank_metric = "rmse") %>%
 select(modelo = wflow_id, .metric, mean, rank) %>%
 pivot_wider(names_from = .metric, values_from = mean) |>
 relocate(modelo, rank)
```

```{r}
best_neural <- tune_rank_neural |> 
 pull(modelo) %>%
 set_names(.)
```

```{r}
mejor_neural <- neural_res |> 
 extract_workflow_set_result(id = "rec_all_neural") |> 
 select_best(metric = "rmse")
```

Antes de realizar el ajuste (*fit*) definitivo con `last_fit()` en los datos
de prueba, vamos a ajustar el modelo con el fin de obtener un objeto de clase
**nn** que nos servirá para realizar los gráficos que interesan

```{r}
tic()
neural_samfit <- neural_res |> 
 extract_workflow(id = best_neural) |> 
 finalize_workflow(mejor_neural) |> 
 fit(autos_train)
toc()
```

```{r}
neural_nativo <- extract_fit_engine(neural_samfit)
```

```{r}
names(neural_nativo)
```

```{r}
plotnet(neural_nativo)
```



```{r}
tic()
test_neural_res <- neural_res	|> 
 extract_workflow(id = best_neural) |> 
 finalize_workflow(mejor_neural) |> 
 last_fit(split = autos_split, metrics = metricas_neural)
toc()
```


```{r}
modelo_neuralnet <- extract_fit_engine(neu_res)
```

```{r}
olden(neural_ajuste)
```

```{r}
# lekprofile(neural_ajuste, group_show = TRUE)
```



































### nnets

Hay que tener en cuenta que parsnip establece automáticamente la activación
lineal en la última capa, es decir, para el caso de *nnet* no es posible ajustar
la función de activación.

```{r}
mlp_nnets <- mlp(hidden_units = tune(),
                 penalty      = 0.1,
                 epochs       = tune()) |>
	set_engine("nnet") %>%
	set_mode("regression")
```

Los parámetros específicos del motor utilizado se especifican en la parte de
`set_engine`.

```{r}
mlp_nnets
```


```{r}
autos_workflow <- workflow_set(preproc = list(rec_all = uni_rec),
										 models  = list(nnet = mlp_nnets))
```

```{r}
mset <- metric_set(rmse, rsq)
```


```{r}
race_ctrl <- control_race(
 	save_pred     = TRUE, 
 	parallel_over = "everything",
 	save_workflow = TRUE)
```


```{r, eval=FALSE}
all_cores <- detectCores(logical = FALSE)
clusterpr <- makePSOCKcluster(all_cores)
registerDoParallel(clusterpr)
```

```{r, echo=FALSE, eval=FALSE}
pin_versions(tablero, "nnets_res")
```

```{r, echo=FALSE, eval=FALSE}
# ver cual versión debo cargar
ver <- pin_versions(tablero, "nnets_res")$version[[1]]
pin_meta(tablero, "nnets_res", version = ver)$description
```

```{r, echo=FALSE}
nnets_res <- pin_read(board = tablero, name = "nnets_res", version = ver)
```


```{r, eval=FALSE}
# 20.36 seg
tic()
nnets_res <- autos_workflow |> 
	workflow_map(
		"tune_race_anova",
		seed = 1503,
		resamples = autos_folds,
		verbose = TRUE,
		metrics = mset,
		grid    = 20, 
		control = race_ctrl)
toc()
```

```{r, eval=FALSE}
stopCluster(clusterpr)
unregister()
```

```{r, eval=FALSE, echo=FALSE}
pin_write(board = tablero,
			 x     = nnets_res,
			 name  = "nnets_res",
			 type  = "rds",
			 title = "nnets",
			 description = "Modelo nnets")
```

Veamos cuantos modelos fueron ajustados:

```{r}
(tm <- nrow(collect_metrics(nnets_res, summarize = FALSE)))
```

Ahora veamos los resultados de las métricas con nuestro conjunto de entrenamiento.

```{r}
tune_rank <- nnets_res %>%
 rank_results(select_best = TRUE, rank_metric = "rmse") %>%
 select(modelo = wflow_id, .metric, mean, rank) %>%
 pivot_wider(names_from = .metric, values_from = mean) |>
 relocate(modelo, rank)
```

```{r}
tune_rank %>%
 tabla(cap = "Ranking de los mejores ajustes con datos de entrenamiento")
```

```{r}
autoplot(tune_res, select_best = TRUE, rank_metric = "rmse") + drako
```

```{r}
best <- tune_rank |> 
 pull(modelo) %>%
 set_names(.)
```

```{r}
mejor_nnet <- nnets_res |> 
 extract_workflow_set_result(id = "rec_all_nnet") |> 
 select_best(metric = "rmse")
```

```{r}
tic()
test_res <- nnets_res	|> 
 extract_workflow(id = best) |> 
 finalize_workflow(mejor_nnet) |> 
 last_fit(split = autos_split, metrics = mset)
toc()
```

```{r}
metricas_test <- test_res %>% collect_metrics() |> 
 pivot_wider(names_from = .metric, values_from = .estimate) |> 
 select(-c(.estimator:.config))
```

```{r}
metricas_test |> tabla("Métricas con el conjunto de prueba")
```



## Resultados




# Referencias
